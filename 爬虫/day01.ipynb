{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab7fed0-eb68-46d2-8d6f-58dde6608059",
   "metadata": {},
   "source": [
    "B站爬虫课程：https://www.bilibili.com/video/BV1ha4y1H7sx/?spm_id_from=333.337.search-card.all.click&vd_source=f093c3d64ba399e149cbffa6cd31a7b0\n",
    "\n",
    "爬虫：通过编写程序，模拟浏览器上网然后让其去互联网上抓取数据的过程。\n",
    "爬虫的分类：\n",
    "1.通用爬虫：抓取系统重要的组成部分，抓取一张页面数据\n",
    "2.聚焦爬虫：建立在通用爬虫基础之上，抓取页面中特定的局部内容\n",
    "3.增量式爬虫：检测网站中数据更新的情况，只会爬取最新更新出来的数据\n",
    "\n",
    "http协议：\n",
    "服务器和客户端进行交流的一种交互方式——握手、交互信息。\n",
    "常用的请求头信息：\n",
    "User-Agent:请求载体的身份表示 \n",
    "Connection:请求完毕后，是断开还是连接\n",
    "常用的响应头信息：\n",
    "Content-Type:服务器回应客户端的数据类型\n",
    "https协议：\n",
    "安全的超文本传输协议\n",
    "数据的加密方式：\n",
    "1.对秘钥的加密\n",
    "2.非对称证书的加密\n",
    "3.证书秘钥加密\n",
    "Cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01dcd7f-ce5b-4ffb-86ee-4c8f81860570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd745d7-f48d-4d49-a74f-3b910afd7c4f",
   "metadata": {},
   "source": [
    "requests操作流程：request为通用爬虫，会爬取页面的一页数据\n",
    "1.指定url\n",
    "2.发起请求\n",
    "3.获取相应数据\n",
    "4.持久化储存\n",
    "page_data = re.text-返回utf-8形式的数据-str\n",
    "page_data = re.content-返回二进制形式的数据-bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ff1c4b-a362-47f3-9856-40f06f1cd5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html>\\n<head>\\n<title>Access Denied</title>\\n</head>\\n<body>\\n<h1>:(</h1>\\n<p></p>\\n<hr>\\n<p>Access Denied</p>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#指定url\n",
    "url = \"https://www.sougou.com/\"\n",
    "##对于一些反爬设置User-Agent与Cookie\n",
    "\n",
    "# headers={\n",
    "#     'User-Agent':'',\n",
    "#     'Cookie':''\n",
    "# }\n",
    "#翻页爬取需要加页数参数\n",
    "# para = {\n",
    "#     kw = \n",
    "# }\n",
    "#发起请求\n",
    "re = requests.get(url=url)\n",
    "# re = request.get(url=url,headers=headers,para=para)\n",
    "\n",
    "#获取相应数据\n",
    "#page_text = re.content\n",
    "page_text = re.text\n",
    "#utf-8数据转换成ascii\n",
    "re.encoding = re.apparent_encoding\n",
    "#Beautiful解析html网页\n",
    "bs = BeautifulSoup(re.text,'html.parser')\n",
    "re.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2d552-f62e-4821-8d5e-b22dc03df50d",
   "metadata": {},
   "source": [
    "聚焦爬虫：运用数据解析提取特定数据内容；建立在通用爬虫基础之上。\n",
    "数据解析分类：\n",
    "1.正则表达式\n",
    "2.bs4\n",
    "3.xpath（重点学习，通用性强兼容性强-scrapy）\n",
    "解析的局部文本数据都是存放在标签之间或者标签对应的属性中进行存储\n",
    "1.首先对标签进行定位2.标签或者标签对应的属性中存储的数据值进行提取\n",
    "聚焦爬虫流程：\n",
    "1.指定url\n",
    "2.发起请求\n",
    "3.获取相应数据\n",
    "4.数据解析\n",
    "5.局部数据持久化储存"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "906cbab1",
   "metadata": {},
   "source": [
    "运用正则表达式爬取页面图片\n",
    "正则表达式的运用需要注意的是.*?的匹配的原则\n",
    "特定标签div的结构分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06556b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.指定url\n",
    "url = \"https://www.bilibili.com/video/BV1ha4y1H7sx/?p=20&spm_id_from=pageDriver&vd_source=f093c3d64ba399e149cbffa6cd31a7b0\"\n",
    "headers = {\n",
    "    \"User-Agent\":\"https://www.bilibili.com/video/BV1ha4y1H7sx/?p=20&spm_id_from=pageDriver&vd_source=f093c3d64ba399e149cbffa6cd31a7b0\",\n",
    "    \"Cookie\":\"https://www.bilibili.com/video/BV1ha4y1H7sx/?p=20&spm_id_from=pageDriver&vd_source=f093c3d64ba399e149cbffa6cd31a7b0\"\n",
    "    \n",
    "}\n",
    "#2.发送请求\n",
    "response = requests.get(url=url,headers=headers)\n",
    "#获取相应数据\n",
    "page_txt = response.text\n",
    "#得到了一个长的字符串\n",
    "#3.进行数据解析\n",
    "com = '<div class=\"root-reply\" data-v-6376d2f8=\"\"><span class=\"reply-content root-reply\" data-v-6376d2f8=\"\">.*?</div>'\n",
    "#<span class=\"reply-content sub-reply-content\" data-v-15f04a69=\"\">请问可以爬原动力文档下载吗</span>\n",
    "#re.S单行匹配->数据解析使用/re.M多行匹配\n",
    "com_lis = re.findall(com,page_txt,re.S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "088366ae",
   "metadata": {},
   "source": [
    "BeautifulSoup的使用：\n",
    "遍历文档树->html页面就是一颗大的标签文档树，bs4就是去解析这个文档树(标签定位)\n",
    "解析原理：\n",
    "1.实例化一个bs4对象，将页面的源代码加载到该对象中\n",
    "fp = open(\"html的文件路径\"，'r',encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(fp,\"lxml\")\n",
    "2.通过调用BeautifulSoup中的相关方法进行特定标签和数据的提取\n",
    "page_txt = response.text\n",
    "soup = BeautifulSoup(page_txt,\"lxml\")\n",
    "soup.tagname返回的是第一次出现的特定标签==soup.find('tagname');soup还可以进行属性定位：soup.find('tagname',class_=\"属性名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2e87c6-f1ad-48b4-b29c-af48e298d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BeautifulSoup的两种解析方式：1.解析本地html文件2.解析web网页中的html\n",
    "#1.指定url\n",
    "Url = \"https://www.baidu.com/\"\n",
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\",\n",
    "    \"Cookie\":\"\"\n",
    "}\n",
    "#2.发送请求\n",
    "re = requests.get(url=Url,headers=headers)\n",
    "#获取相应数据\n",
    "p_txt = re.text\n",
    "#soup = BeautifulSoup(p_txt,'html.parser')\n",
    "soup = BeautifulSoup(p_txt,\"lxml\")\n",
    "#soup.tagname返回的是第一次出现的特定标签\n",
    "soup.find('div',class_=\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1105a2c",
   "metadata": {},
   "source": [
    "soup.find('div',class_=\"song\")依据属性的特定标签选取\n",
    "soup.select('某种选择器(id,calss,标签...选择器)')返回一个列表\n",
    "soup.select('.tang'):返回一个列表数据;\n",
    "select也支持层级访问标签:soup.select('.tang > ul > li > a')[0]\n",
    "select用空格实现跨级访问：soup.select('.tang > ul a')\n",
    "获取标签中的文本数据：soup.select('.tang > ul > li > a')[0].text返回所有文本/string返回直系文本/get_text()\n",
    "获取标签当中的属性值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b4d9efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.song.com/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = open(\"test.html\",'r',encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(fp,\"lxml\")\n",
    "soup.find('div',class_=\"song\")\n",
    "# soup.select('.tang > ul > li > a')[0]\n",
    "# soup.select('.tang > ul a').get_text()\n",
    "soup.find('div',class_=\"song\").get_text()\n",
    "soup.a['href']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
